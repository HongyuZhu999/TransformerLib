{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8e5219-ad72-46bc-a7fa-65a733f5a3af",
   "metadata": {},
   "source": [
    "# GPT\n",
    "## Introduction to GPT\n",
    "</br> GPT is a model developed based on the Transformer architecture and focuses on generating pre-training tasks. GPT uses the decoder part of Transformer for autoregressive pre-training, and generates text by learning a large amount of text data.\n",
    "</br> For specific tasks, Transformer models usually need to be trained from scratch. GPT uses **Pre-training & Fine-tuning**. First, it is pre-trained on large-scale unlabeled text data to learn general patterns and knowledge of language; then, it can be adapted to specific tasks or applications by fine-tuning on smaller task-specific data sets.\n",
    "</br> Since GPT is a pre-trained model, it is particularly effective in handling language generation tasks, such as text generation, dialogue systems, content creation assistance, etc. The pre-training nature of GPT also makes it excellent at understanding complex language patterns and context. At the same time, due to its extensive pre-training, GPT is able to generalize to a wide range of tasks with little or no task-specific training data, although fine-tuning can further improve its performance on specific tasks.\n",
    "### Framework of GPT\n",
    "<img src = https://d3i71xaburhd42.cloudfront.net/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035/4-Figure1-1.png width = 800>\n",
    "Figure: (left) Transformer architecture and training objectives used in this work. (right) Input transformations for fine-tuning on different tasks\n",
    "\n",
    "- Unsupervised pre-training: Use a multi-layer Transformer decoder for the language model, which is a variant of the transformer. Train with unsupervised corpus of tokens.\n",
    "- Supervised fine-tuning: Use parameters to the supervised target task.\n",
    "- Task-specific input transformations: A traversal-style approach.\n",
    "### Development of GPT\n",
    "- GPT-1: Released in 2018, it contains 12 layers of Transformer and has 110 million parameters.\n",
    "- GPT-2: The model size has been greatly increased and different versions are provided. The largest version contains 1542 layers of Transformer and has 1.5 billion parameters.\n",
    "- GPT-3: Compared with GPT-2, the model size, amount of pre-training data and used pre-training tasks have increased. Including texts, books, news, Wikipedia, etc. on the Internet.\n",
    "- InstructGPT: Making language models larger does not mean that they are better able to follow user intentions, so the main problem to solve is how to make language models better able to follow instructions given by humans and implement them in practice.\n",
    "- ChatGPT: OpenAI's upgraded version based on the GPT-3 model in 2022 is mainly optimized for dialogue tasks, adding input and output of dialogue history, and control of dialogue strategies.\n",
    "- GPT-4: In the conversation task, the difference between GPT4 and GPT-3.5 or GPT3 is not big. But when the complexity of the task reaches a sufficient threshold, differences emerge, and GPT-4 is more reliable, more creative, and able to handle more nuanced instructions.\n",
    "## Reference\n",
    "1. GPT-1 \"[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\" by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskeve\n",
    "2. \"[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)\" by Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee and Luke Zettlemoyer.r3\n",
    "2. GPT-2 \"[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\" by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and \n",
    "Ilya Sutskeve4\n",
    "3. GPT-3 \"[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\" by Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah and Jared Kaplan, etc5\n",
    "4. InstructGPT: \"[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\" by Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, etc.\n",
    "5. GPT-4 \"[GPT-4 Technical Report](https://arxiv.org/abs/2303.08774v2)\" by OpenAI.\n",
    "6. \"[GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models](https://arxiv.org/abs/2303.10130)\" by Tyna Eloundou, Sam Manning, Pamela Mishkin and Daniel Rock.1\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d458b40-0dc2-4e03-9491-48e207c8e5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He worked as a security guard at the airport for the past two years.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# The input text\n",
    "text = \"He worked as a\"\n",
    "\n",
    "# Encoder\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    text, \n",
    "    return_tensors='pt', \n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "# Generater\n",
    "output_sequences = model.generate(\n",
    "    input_ids=encoded_input['input_ids'],\n",
    "    attention_mask=encoded_input['attention_mask'],\n",
    "    max_length=16, \n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=model.config.eos_token_id\n",
    ")\n",
    "\n",
    "# Decoder\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d19f17-e57d-487d-907e-4cb0cf66ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He worked as a security guard at the airport for the past two years.\n",
      "\n",
      "\"I'm not sure if he's a terrorist or not,\" said a source close to the investigation. \"He's a very good person. He's a very good person. He's a very good person.\"\n",
      "\n",
      "The source said the FBI is still looking into the case.\n",
      "\n",
      "The FBI is also looking into the possibility that the man was involved in a terrorist attack in New York City.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# The input text\n",
    "text = \"He worked as a\"\n",
    "\n",
    "# Encoder\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    text, \n",
    "    return_tensors='pt', \n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "# Generater\n",
    "output_sequences = model.generate(\n",
    "    input_ids=encoded_input['input_ids'],\n",
    "    attention_mask=encoded_input['attention_mask'],\n",
    "    max_length=100, \n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=model.config.eos_token_id\n",
    ")\n",
    "\n",
    "# Decoder\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5e2ad6-1c56-4527-a3d0-bcba48a195a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob is good boy, but he's not a good boy. He's a bad boy. He's a bad boy. He's a bad boy. He's a bad boy. He's a bad boy. He's a bad boy. He\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# The input text\n",
    "text = \"Bob is good boy, but\"\n",
    "\n",
    "# Encoder\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    text, \n",
    "    return_tensors='pt', \n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "# Generater\n",
    "output_sequences = model.generate(\n",
    "    input_ids=encoded_input['input_ids'],\n",
    "    attention_mask=encoded_input['attention_mask'],\n",
    "    max_length=50, \n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=model.config.eos_token_id\n",
    ")\n",
    "\n",
    "# Decoder\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
